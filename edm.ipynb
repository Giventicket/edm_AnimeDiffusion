{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDM hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "- sigma_min = 0.002\n",
    "- sigma_max = 80\n",
    "- sigma_data = 0.5\n",
    "- rho = 7\n",
    "- P_mean =−1.2\n",
    "- P_std = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDM Model Forward 참고자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@persistence.persistent_class\n",
    "class EDMPrecond(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        img_resolution,                     # Image resolution.\n",
    "        img_channels,                       # Number of color channels.\n",
    "        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n",
    "        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n",
    "        sigma_min       = 0,                # Minimum supported noise level.\n",
    "        sigma_max       = float('inf'),     # Maximum supported noise level.\n",
    "        sigma_data      = 0.5,              # Expected standard deviation of the training data.\n",
    "        model_type      = 'DhariwalUNet',   # Class name of the underlying model.\n",
    "        **model_kwargs,                     # Keyword arguments for the underlying model.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_resolution = img_resolution\n",
    "        self.img_channels = img_channels\n",
    "        self.label_dim = label_dim\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_max = sigma_max\n",
    "        self.sigma_data = sigma_data\n",
    "        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n",
    "\n",
    "    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n",
    "        x = x.to(torch.float32)\n",
    "        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n",
    "        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n",
    "        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n",
    "\n",
    "        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n",
    "        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n",
    "        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n",
    "        c_noise = sigma.log() / 4\n",
    "\n",
    "        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n",
    "        assert F_x.dtype == dtype\n",
    "        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n",
    "        return D_x\n",
    "\n",
    "    def round_sigma(self, sigma):\n",
    "        return torch.as_tensor(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@persistence.persistent_class\n",
    "class EDMLoss:\n",
    "    def __init__(self, P_mean=-1.2, P_std=1.2, sigma_data=0.5):\n",
    "        self.P_mean = P_mean\n",
    "        self.P_std = P_std\n",
    "        self.sigma_data = sigma_data\n",
    "\n",
    "    def __call__(self, net, images, labels=None, augment_pipe=None):\n",
    "        rnd_normal = torch.randn([images.shape[0], 1, 1, 1], device=images.device)\n",
    "        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n",
    "        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n",
    "        y, augment_labels = augment_pipe(images) if augment_pipe is not None else (images, None)\n",
    "        n = torch.randn_like(y) * sigma\n",
    "        D_yn = net(y + n, sigma, labels, augment_labels=augment_labels)\n",
    "        loss = weight * ((D_yn - y) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@persistence.persistent_class\n",
    "class DhariwalUNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        img_resolution,                     # Image resolution at input/output.\n",
    "        in_channels,                        # Number of color channels at input.\n",
    "        out_channels,                       # Number of color channels at output.\n",
    "        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n",
    "        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n",
    "\n",
    "        model_channels      = 192,          # Base multiplier for the number of channels.\n",
    "        channel_mult        = [1,2,3,4],    # Per-resolution multipliers for the number of channels.\n",
    "        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n",
    "        num_blocks          = 3,            # Number of residual blocks per resolution.\n",
    "        attn_resolutions    = [32,16,8],    # List of resolutions with self-attention.\n",
    "        dropout             = 0.10,         # List of resolutions with self-attention.\n",
    "        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.label_dropout = label_dropout\n",
    "        emb_channels = model_channels * channel_mult_emb\n",
    "        init = dict(init_mode='kaiming_uniform', init_weight=np.sqrt(1/3), init_bias=np.sqrt(1/3))\n",
    "        init_zero = dict(init_mode='kaiming_uniform', init_weight=0, init_bias=0)\n",
    "        block_kwargs = dict(emb_channels=emb_channels, channels_per_head=64, dropout=dropout, init=init, init_zero=init_zero)\n",
    "\n",
    "        # Mapping.\n",
    "        self.map_noise = PositionalEmbedding(num_channels=model_channels)\n",
    "        self.map_augment = Linear(in_features=augment_dim, out_features=model_channels, bias=False, **init_zero) if augment_dim else None\n",
    "        self.map_layer0 = Linear(in_features=model_channels, out_features=emb_channels, **init)\n",
    "        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n",
    "        self.map_label = Linear(in_features=label_dim, out_features=emb_channels, bias=False, init_mode='kaiming_normal', init_weight=np.sqrt(label_dim)) if label_dim else None\n",
    "\n",
    "        # Encoder.\n",
    "        self.enc = torch.nn.ModuleDict()\n",
    "        cout = in_channels\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            res = img_resolution >> level\n",
    "            if level == 0:\n",
    "                cin = cout\n",
    "                cout = model_channels * mult\n",
    "                self.enc[f'{res}x{res}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n",
    "            else:\n",
    "                self.enc[f'{res}x{res}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n",
    "            for idx in range(num_blocks):\n",
    "                cin = cout\n",
    "                cout = model_channels * mult\n",
    "                self.enc[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n",
    "        skips = [block.out_channels for block in self.enc.values()]\n",
    "\n",
    "        # Decoder.\n",
    "        self.dec = torch.nn.ModuleDict()\n",
    "        for level, mult in reversed(list(enumerate(channel_mult))):\n",
    "            res = img_resolution >> level\n",
    "            if level == len(channel_mult) - 1:\n",
    "                self.dec[f'{res}x{res}_in0'] = UNetBlock(in_channels=cout, out_channels=cout, attention=True, **block_kwargs)\n",
    "                self.dec[f'{res}x{res}_in1'] = UNetBlock(in_channels=cout, out_channels=cout, **block_kwargs)\n",
    "            else:\n",
    "                self.dec[f'{res}x{res}_up'] = UNetBlock(in_channels=cout, out_channels=cout, up=True, **block_kwargs)\n",
    "            for idx in range(num_blocks + 1):\n",
    "                cin = cout + skips.pop()\n",
    "                cout = model_channels * mult\n",
    "                self.dec[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n",
    "        self.out_norm = GroupNorm(num_channels=cout)\n",
    "        self.out_conv = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n",
    "\n",
    "    def forward(self, x, noise_labels, class_labels, augment_labels=None):\n",
    "        # Mapping.\n",
    "        emb = self.map_noise(noise_labels)\n",
    "        if self.map_augment is not None and augment_labels is not None:\n",
    "            emb = emb + self.map_augment(augment_labels)\n",
    "        emb = silu(self.map_layer0(emb))\n",
    "        emb = self.map_layer1(emb)\n",
    "        if self.map_label is not None:\n",
    "            tmp = class_labels\n",
    "            if self.training and self.label_dropout:\n",
    "                tmp = tmp * (torch.rand([x.shape[0], 1], device=x.device) >= self.label_dropout).to(tmp.dtype)\n",
    "            emb = emb + self.map_label(tmp)\n",
    "        emb = silu(emb)\n",
    "\n",
    "        # Encoder.\n",
    "        skips = []\n",
    "        for block in self.enc.values():\n",
    "            x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        # Decoder.\n",
    "        for block in self.dec.values():\n",
    "            if x.shape[1] != block.in_channels:\n",
    "                x = torch.cat([x, skips.pop()], dim=1)\n",
    "            x = block(x, emb)\n",
    "        x = self.out_conv(silu(self.out_norm(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train EDM 참고자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ema_halflife_kimg   = 500,      # Half-life of the exponential moving average (EMA) of model weights.\n",
    "ema_rampup_ratio    = 0.05,     # EMA ramp-up coefficient, None = no rampup.\n",
    "lr_rampup_kimg      = 10000,    # Learning rate ramp-up duration.\n",
    "loss_scaling        = 1,        # Loss scaling factor for reducing FP16 under/overflows.\n",
    "resume_kimg         = 0,        # Start from the given training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dist.print0(f'Training for {total_kimg} kimg...')\n",
    "    dist.print0()\n",
    "    cur_nimg = resume_kimg * 1000\n",
    "    cur_tick = 0\n",
    "    tick_start_nimg = cur_nimg\n",
    "    tick_start_time = time.time()\n",
    "    maintenance_time = tick_start_time - start_time\n",
    "    dist.update_progress(cur_nimg // 1000, total_kimg)\n",
    "    stats_jsonl = None\n",
    "    while True:\n",
    "\n",
    "        # Accumulate gradients.\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        for round_idx in range(num_accumulation_rounds):\n",
    "            with misc.ddp_sync(ddp, (round_idx == num_accumulation_rounds - 1)):\n",
    "                images, labels = next(dataset_iterator)\n",
    "                images = images.to(device).to(torch.float32) / 127.5 - 1\n",
    "                labels = labels.to(device)\n",
    "                loss = loss_fn(net=ddp, images=images, labels=labels, augment_pipe=augment_pipe)\n",
    "                training_stats.report('Loss/loss', loss)\n",
    "                loss.sum().mul(loss_scaling / batch_gpu_total).backward()\n",
    "\n",
    "        # Update weights.\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = optimizer_kwargs['lr'] * min(cur_nimg / max(lr_rampup_kimg * 1000, 1e-8), 1)\n",
    "        for param in net.parameters():\n",
    "            if param.grad is not None:\n",
    "                torch.nan_to_num(param.grad, nan=0, posinf=1e5, neginf=-1e5, out=param.grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update EMA.\n",
    "        ema_halflife_nimg = ema_halflife_kimg * 1000\n",
    "        if ema_rampup_ratio is not None:\n",
    "            ema_halflife_nimg = min(ema_halflife_nimg, cur_nimg * ema_rampup_ratio)\n",
    "        ema_beta = 0.5 ** (batch_size / max(ema_halflife_nimg, 1e-8))\n",
    "        for p_ema, p_net in zip(ema.parameters(), net.parameters()):\n",
    "            p_ema.copy_(p_net.detach().lerp(p_ema, ema_beta))\n",
    "\n",
    "        # Perform maintenance tasks once per tick.\n",
    "        cur_nimg += batch_size\n",
    "        done = (cur_nimg >= total_kimg * 1000)\n",
    "        if (not done) and (cur_tick != 0) and (cur_nimg < tick_start_nimg + kimg_per_tick * 1000):\n",
    "            continue\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        # Update state.\n",
    "        cur_tick += 1\n",
    "        tick_start_nimg = cur_nimg\n",
    "        tick_start_time = time.time()\n",
    "        maintenance_time = tick_start_time - tick_end_time\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDM Sampling 참고자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def edm_sampler(\n",
    "    net, latents, class_labels=None, randn_like=torch.randn_like,\n",
    "    num_steps=18, sigma_min=0.002, sigma_max=80, rho=7,\n",
    "    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1,\n",
    "):\n",
    "    # Adjust noise levels based on what's supported by the network.\n",
    "    sigma_min = max(sigma_min, net.sigma_min)\n",
    "    sigma_max = min(sigma_max, net.sigma_max)\n",
    "\n",
    "    # Time step discretization.\n",
    "    step_indices = torch.arange(num_steps, dtype=torch.float64, device=latents.device)\n",
    "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    t_steps = torch.cat([net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
    "\n",
    "    # Main sampling loop.\n",
    "    x_next = latents.to(torch.float64) * t_steps[0]\n",
    "    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n",
    "        x_cur = x_next\n",
    "\n",
    "        # Increase noise temporarily.\n",
    "        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n",
    "        t_hat = net.round_sigma(t_cur + gamma * t_cur)\n",
    "        x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * randn_like(x_cur)\n",
    "\n",
    "        # Euler step.\n",
    "        denoised = net(x_hat, t_hat, class_labels).to(torch.float64)\n",
    "        d_cur = (x_hat - denoised) / t_hat\n",
    "        x_next = x_hat + (t_next - t_hat) * d_cur\n",
    "\n",
    "        # Apply 2nd order correction.\n",
    "        if i < num_steps - 1:\n",
    "            denoised = net(x_next, t_next, class_labels).to(torch.float64)\n",
    "            d_prime = (x_next - denoised) / t_next\n",
    "            x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "\n",
    "    return x_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
